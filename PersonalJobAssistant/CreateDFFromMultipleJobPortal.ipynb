{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "import re\n",
    "from Dynamic_Scrape import Scrape_html\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import urllib2\n",
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['Salary', 'Industry', 'Functional Area', 'Job Title', 'Role','Employment Type']\n",
    "edu_labels = ['UG', 'PG', 'Doctorate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CreateDataFrameFromSource(object):\n",
    "    \n",
    "    #parse the job data \n",
    "    \n",
    "    def __init__(self,url,name,keyword,source):\n",
    "        self.name = name\n",
    "        self.source = source\n",
    "        self.keyword= keyword\n",
    "        self.url = url\n",
    "        self.ndf = pd.DataFrame()\n",
    "        self.tdf = pd.DataFrame()\n",
    "        self.mdf = pd.DataFrame()\n",
    "        \n",
    "        if self.source=='naukri':\n",
    "            \n",
    "            html = self.dynamic_scrape(url,name,keyword)\n",
    "            #print html\n",
    "            soup= BeautifulSoup(html,'lxml') \n",
    "            self.ndf =self.getNaukriDF(soup)\n",
    "        elif self.source =='times':\n",
    "            html = self.dynamic_scrape(url,name,keyword)\n",
    "            soup= BeautifulSoup(html,'lxml') \n",
    "            #print(soup.prettify())\n",
    "            self.tdf =self.getTimesDF(soup)\n",
    "        elif self.source =='monster':\n",
    "            html = self.dynamic_scrape(url,name,keyword)\n",
    "            soup= BeautifulSoup(html,'lxml') \n",
    "            print(soup.prettify())\n",
    "            self.tdf =self.getMonsterDF(soup)  \n",
    "        else:\n",
    "            print('Source not reachable')\n",
    "            \n",
    "    def dynamic_scrape(self,url,name,keyword):\n",
    "        dynamic_scrape = Scrape_html()\n",
    "        html = dynamic_scrape.get_html(url,name,keyword)\n",
    "        return(html)\n",
    "    def parseJobData(self,jd_soup):\n",
    "        try:\n",
    "            #url \n",
    "            url = jd_soup.find(\"a\",{\"itemprop\":\"url\"}).getText().strip()\n",
    "            \n",
    "            #job descripttion\n",
    "            jd_text = jd_soup.find(\"ul\",{\"itemprop\":\"description\"}).getText().strip()\n",
    "\n",
    "            #job location\n",
    "            location = jd_soup.find(\"div\",{\"class\":\"loc\"}).getText().strip()\n",
    "\n",
    "            # Experience\n",
    "            experience = jd_soup.find(\"span\",{\"itemprop\":\"experienceRequirements\"}).getText().strip()\n",
    "\n",
    "            role_info = [content.getText().split(':')[-1].strip() for content in jd_soup.find(\"div\",{\"class\":\"jDisc mt20\"}).contents if len(str(content).replace(' ',''))!=0]\n",
    "            role_info_dict = {label: role_info for label, role_info in zip(labels, role_info)}\n",
    "\n",
    "            #key Skills\n",
    "            key_skills = '|'.join(jd_soup.find(\"div\",{\"class\":\"ksTags\"}).getText().split('  '))[1:]\n",
    "\n",
    "            #edu_info\n",
    "            edu_info = [content.getText().split(':') for content in jd_soup.find(\"div\",{\"itemprop\":\"educationRequirements\"}).contents if len(str(content).replace(' ',''))!=0]\n",
    "\n",
    "\n",
    "            #print (edu_info)\n",
    "\n",
    "            edu_info_dict = {label.strip(): edu_info.strip() for label, edu_info in edu_info}\n",
    "            for l in edu_labels:\n",
    "                if l not in edu_info_dict.keys():\n",
    "                    edu_info_dict[l] = ''\n",
    "\n",
    "            company_name = jd_soup.find(\"div\",{\"itemprop\":\"hiringOrganization\"}).contents[1].p.getText().strip()\n",
    "        except AttributeError:\n",
    "            return -1\n",
    "        df_dict = OrderedDict({'Location':location, 'Link':url,'Job Description':jd_text,'Experience':experience,'Skills':key_skills,'Company Name':company_name})\n",
    "        df_dict.update(role_info_dict)\n",
    "        df_dict.update(edu_info_dict)  \n",
    "        #time.sleep(1)\n",
    "        return df_dict\n",
    "    \n",
    "    def getNaukriDF(self, soup):\n",
    "        #https://www.naukri.com/c-jobs retrieve page wise data\n",
    "        \n",
    "        base_url = soup.link[\"href\"]\n",
    "        \n",
    "        print base_url\n",
    "            \n",
    "        num_jobs = int(soup.find(\"div\", { \"class\" : \"count\" }).h1.contents[1].getText().split(' ')[-1])\n",
    "        \n",
    "        \n",
    "        #extract number of pages as 50 job per page \n",
    "       \n",
    "        #number of pages\n",
    "        num_pages = int(math.ceil(num_jobs/50.0))\n",
    "        #print total number of page\n",
    "        print ('Total Number of jobs  :{}'.format(num_pages))\n",
    "        if num_pages > 30:\n",
    "            num_pages=5\n",
    "            \n",
    "        print ('Total Number of jobs  :{}'.format(num_pages))\n",
    "        \n",
    "         #for all the link retrieve the data \n",
    "        naukri_df = pd.DataFrame()\n",
    "        \n",
    "        for page in range(1,num_pages): #replace  ith numpages+1 \n",
    "            page_url = base_url+str(page)\n",
    "            req = urllib2.Request(page_url, headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "            source = urllib2.urlopen( req ).read()\n",
    "            soup = BeautifulSoup(source,\"html.parser\")\n",
    "        \n",
    "            all_links = [link.get('href') for link in soup.findAll('a') if 'job-listings' in  str(link.get('href'))]\n",
    "\n",
    "            #print (all_links)\n",
    "\n",
    "           \n",
    "        \n",
    "            for url in all_links:\n",
    "\n",
    "                req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "                #job descrption data open the link in the browser\n",
    "                jd_source = urllib2.urlopen(req).read()\n",
    "\n",
    "                jd_soup = BeautifulSoup(jd_source,\"html.parser\")\n",
    "\n",
    "                #parse job data\n",
    "        \n",
    "            \n",
    "                if(self.parseJobData(jd_soup)==-1):\n",
    "                    continue\n",
    "                else:\n",
    "                    naukri_df = naukri_df.append(self.parseJobData(jd_soup),ignore_index=True) \n",
    "                    \n",
    "            print(page)\n",
    "                \n",
    "        return naukri_df\n",
    "    \n",
    "    def getTimesDF(self, soup):\n",
    "        times_df=pd.DataFrame()\n",
    "        #r=requests.get(times_url)\n",
    "        #soup=BeautifulSoup(r.content,\"lxml\")\n",
    "        all_links=[]\n",
    "        for link in soup.select('h2 a[href]'):\n",
    "            if \"http\" in link.get(\"href\"):\n",
    "                all_links.append(link['href'])\n",
    "        links=list(set(all_links))\n",
    "\n",
    "        for line in links:\n",
    "            r = requests.get(line)\n",
    "            soup=BeautifulSoup(r.content,\"lxml\")\n",
    "            dict_object={}\n",
    "            dict_object[\"Link\"] = line\n",
    "            desc=soup.find_all(\"section\",{\"id\":\"applyFlowHideDetails_2\"})\n",
    "            for item in desc:\n",
    "                dict_object[\"Job Description\"]= item.text.replace(\"\\n\", \"\").replace(\"Job Description\",\"\")\n",
    "            desc=soup.find_all(\"div\",{\"class\":\"jd-comp-name\"})\n",
    "            for item in desc:\n",
    "                #dict_object[\"Company_name\"]= item.text.replace(\"\\n\", \"\")\n",
    "                fields=item.text.replace(\"\\n\", \" \").replace(\"  \",\"|\").split('|')\n",
    "                #print(fields[1])\n",
    "                dict_object[\"Company Name\"]=fields[1]\n",
    "            desc=soup.find_all(\"ul\",{\"class\":\"job-more-dtl clearfix\"})\n",
    "            for item in desc:\n",
    "                fields=item.text.replace(\"\\n\",\" \").replace(\"  \",\"|\").split('|')\n",
    "                dict_object[\"Experience\"]=fields[2]\n",
    "                dict_object[\"Salary\"]=fields[3]\n",
    "                dict_object[\"Location\"]=fields[4]\n",
    "            df=pd.DataFrame([dict_object])\n",
    "            times_df=times_df.append(df,ignore_index=True)\n",
    "        return times_df\n",
    "    \n",
    "    def parseMonsterJobDetails(self,jd_soup):\n",
    "        dict_object = {}\n",
    "\n",
    "        i = str(jd_soup.find_all(\"h1\", { \"class\" : \"job_title_seo\" })); #print(i)\n",
    "        dict_object[\"Job Title\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"/^[ -~]+$/\", \"\")\n",
    "\n",
    "        i = str(jd_soup.find_all(\"a\", { \"class\" : \"keylink lft\" })); #print(i)\n",
    "        dict_object[\"Skills\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \" \").replace(\"/^[ -~]+$/\", \" \")\n",
    "\n",
    "        #i = str(jd_soup.find_all(\"h2\", { \"class\" : \"keyskill skillseotag\" })[0]); #print(i)\n",
    "        #print(re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"/^[ -~]+$/\", \"\"))\n",
    "\n",
    "        i = str(jd_soup.find_all(\"h3\")); #print(i)\n",
    "        dict_object[\"Company Name\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"/^[ -~]+$/\", \"\")\n",
    "\n",
    "        i = str(jd_soup.find_all(\"div\", { \"class\" : \"joblnk\" })[0]); #print(i)\n",
    "        dict_object[\"Location\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"/^[ -~]+$/\", \"\")\n",
    "\n",
    "        i = str(jd_soup.find_all(\"div\", { \"class\" : \"joblnk\" })[1]); #print(i)\n",
    "        dict_object[\"Experience\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"/^[ -~]+$/\", \"\")\n",
    "\n",
    "        #i = str(jd_soup.find_all(\"div\", { \"class\" : \"posted\" })[0]); #print(i)\n",
    "        #dict_object[\"Posted_On\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"/^[ -~]+$/\", \"\")\n",
    "\n",
    "        #i = str(jd_soup.find_all(\"div\", { \"class\" : \"col-md-3 col-xs-12 pull-right jd_rol_section\" })[0]); #print(i)\n",
    "        #dict_object[\"short_desc\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"/^[ -~]+$/\", \"\")\n",
    "\n",
    "        i = str(jd_soup.find_all(\"div\", { \"class\":\"desc\"})[0]); #print(i)\n",
    "        dict_object[\"Job Description\"] = re.sub(re.compile('<.*?>'), '', i).replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\"/^[ -~]+$/\", \"\")\n",
    "\n",
    "        return pd.DataFrame([dict_object])\n",
    "     #CREATE MONSTER DATAFRAME\n",
    "    def getMonsterDF(self, soup):\n",
    "        \n",
    "        monster_df = pd.DataFrame()\n",
    "        base_url = soup.link[\"href\"]\n",
    "        \n",
    "        print base_url\n",
    "            \n",
    "       # num_jobs = int(soup.find(\"div\", { \"class\" : \"count\" }).h1.contents[1].getText().split(' ')[-1])\n",
    "       # print ('Total Number of jobs  :{}'.format(num_jobs))\n",
    "        all_links=[]\n",
    "        \n",
    "        for link in soup.select('link[href]'):\n",
    "            if \"http\" in link.get(\"href\"):\n",
    "                all_links.append(link['href'])\n",
    "        links=list(set(all_links))        \n",
    "        \n",
    "        for line in links:\n",
    "            r = requests.get(line)\n",
    "            soup=BeautifulSoup(r.content,\"lxml\")\n",
    "            \n",
    "            elements = re.findall(r\"sharebutton\\(\\'(.*?)\\?from\", str(soup)) \n",
    "            \n",
    "            for item in elements:\n",
    "                r = requests.get(item)\n",
    "                soup=BeautifulSoup(r.content,\"lxml\")\n",
    "                \n",
    "                monster_df = monster_df.append(self.parseMonsterJobDetails(soup), ignore_index=True)\n",
    "        return monster_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import CreateDFFromMultipleJobPortal\n",
    "#from CreateDFFromMultipleJobPortal import CreateDataFrameFromSource\n",
    "#import sys\n",
    "#del sys.modules['CreateDFFromMultipleJobPortal'] \n",
    "from CreateDFFromMultipleJobPortal import CreateDataFrameFromSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "naukri_url = 'https://www.naukri.com/browse-jobs'\n",
    "name = 'qp'\n",
    "keyword = 'machine learning'\n",
    "\n",
    "#keyword = 'java'\n",
    "#name = 'fts'\n",
    "#monster_url = \"http://www.monsterindia.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ReadDF=CreateDataFrameFromSource(monster_url,name,keyword,'monster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ReadDF.mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
