{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this file contain two classes \n",
    "# 1. Job - Class which takes dataframe path ,take the column and resume text clean  and find the similarity\n",
    "# 2. File - Class to read the file and return the clean tetx at present Implemented only for docx ,\n",
    "# to use this class download as .py and keep in the same folder where the main code cleint code is there and then \n",
    "# import it e.g import Job,import File also below the main code is written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the library \n",
    "\n",
    "#import Panda\n",
    "import pandas as pd\n",
    "# using lemmatization  in place of porter stemmer as gives more meaning base word as compared to stemmer\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "\n",
    "#used for the stop word removal \n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "#regular expression used for unknown chrarater replacing with blank\n",
    "import re \n",
    "#install Package gensim pip install gensim And import the library\n",
    "\n",
    "from gensim import corpora, models, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  getCleanData(text,tokenize=False):\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        txt = str(text)\n",
    "        txt = re.sub(r'[^A-Za-z0-9\\s]',r' ',txt)  # if any other than A-z0-9 Replaced with blank\n",
    "        txt = re.sub(r'\\n',r' ',txt)  #relpace the new line with blank\n",
    "        txt = re.sub(r'\\u',r' ',txt)  #relpace the new line with blank\n",
    "    \n",
    "       #Convert it into lower form\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "       #remove the stop word\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    \n",
    "        #get the base form of word\n",
    "        st = WordNetLemmatizer()\n",
    "        txt = \" \".join([st.lemmatize(w) for w in txt.split()])\n",
    "\n",
    "        if tokenize:\n",
    "            tokens = [token for token in txt.split() ]\n",
    "            return tokens\n",
    "    \n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job:\n",
    "    def __init__(self,dataFrame):\n",
    "         self.job_df=dataFrame\n",
    "         #self.job_df=pd.read_csv(jobDescDataPath)\n",
    "            \n",
    "               \n",
    "     # ****Get Basic Info about Data Frame\n",
    "    def getBasicInfoAboutDataFrame(self):\n",
    "        print(\"\\n\\nData DataTypes:\\n{}\".format(self.job_df.dtypes))\n",
    "        print(\"\\nTrain Shape:\\n{}\".format(self.job_df.shape))\n",
    "        print (\"\\n*********\\n Missing Value Statistics:\\n{}\".format(self.job_df.isnull().sum()))\n",
    "           \n",
    "    def __getCleanDfColumn(self,colname):\n",
    "        print colname\n",
    "        self.job_df[colname+'_Clean'] = self.job_df[colname].map(lambda x: getCleanData(x,True))\n",
    "        self.job_df.head()\n",
    "       # return self.job_df\n",
    "        \n",
    "         #create Bag of Word like count vectorizer\n",
    "    def getSimilarity(self,colname,resumeTextClean):\n",
    "        #get Clean Data column\n",
    "        self.__getCleanDfColumn(colname)\n",
    "        colNameClean=colname+'_Clean'\n",
    "        dictionary = corpora.Dictionary(self.job_df[colNameClean] ) \n",
    "        print dictionary.token2id \n",
    "        corpus = [dictionary.doc2bow(text) for text in self.job_df[colNameClean]]  #count vectorrizer for job description\n",
    "        #count vectorizer for resume text\n",
    "        resume_text_vec=dictionary.doc2bow(resumeTextClean.lower().split()) \n",
    "        #create LSI model fro jobdescription corpus\n",
    "        lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=5)\n",
    "        resume_vec_lsi = lsi[resume_text_vec] # convert the query to LSI space 5 dimension \n",
    "        index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "        simsIndex = index[resume_vec_lsi]  #perform a similarity query against the corpus\n",
    "        simsIndex = sorted(enumerate(simsIndex), key=lambda item: -item[1])\n",
    "        \n",
    "        #verify -print top ten similar document\n",
    "        for i in range(10):\n",
    "            print simsIndex[i]\n",
    "        return simsIndex     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import textract\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ValidExtension=[\"pdf\",\"docx\"]\n",
    "class File(object):\n",
    "    def __init__(self,filename):\n",
    "        self.filename=filename\n",
    "        self.extension=\"\"\n",
    "        #self.getFileExtension() private function\n",
    "    def __getFileExtension(self):\n",
    "        self.extension=self.filename.split('.')[-1]\n",
    "        return self.extension\n",
    "    def __checkExtension(self): ##private function\n",
    "        self.__getFileExtension()\n",
    "        if self.extension not in ValidExtension:\n",
    "            print(\"Error:Not a valid File\")\n",
    "            return False\n",
    "        else: \n",
    "            return True\n",
    "    def readpdfFile(self):  \n",
    "        content=\"\"\n",
    "        print self.filename\n",
    "        pdfFileObj = open(self.filename,'rb')\n",
    "       # read_pdf = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        noofpages = pdfReader.numPages\n",
    "        print(\" Number of Pages {}\".format(noofpages))\n",
    "        #read file \n",
    "        page = pdfReader.getPage(0)\n",
    "        content= page.extractText()\n",
    "      #  text = textract.process(self.filename)\n",
    "       \n",
    "       # print (\"type of page_content {}\".format(type(page_content)))\n",
    "      #  print text  #need to finish\n",
    "        print content.encode('utf-8')\n",
    "    def readdocxFile(self):\n",
    "        resumeCleanText=\"\"\n",
    "        if (self.__checkExtension()):\n",
    "            doc = docx.Document(self.filename)\n",
    "            fullText = []\n",
    "            for para in doc.paragraphs:\n",
    "                fullText.append(para.text)\n",
    "                ' '.join(fullText)\n",
    "            #get clean text\n",
    "            resumeCleanText=getCleanData(fullText)\n",
    "            #return resumeCleanText\n",
    "        else:\n",
    "            print(\"File Extension in Not valid\")          \n",
    "        return resumeCleanText;\n",
    "            \n",
    "           \n",
    "          \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf= File(\"resume.docx\")\n",
    "\n",
    "#Job_df=pd.read_csv(\"Naukri_JD.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume_text=rf.readdocxFile()\n",
    "\n",
    "#Job_df[\"Final_Job_Req\"] = Job_df[\"Location\"]+\" \"+Job_df[\"Experience\"]+\" \"+Job_df[\"Job Description\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pData=Job(Job_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pData.job_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getCleanData\n",
    "#pData.getSimilarity('Final_Job_Req',resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
